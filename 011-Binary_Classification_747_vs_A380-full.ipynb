{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification of 747 vs A380\n",
    "The goal is to classify a picture between two classes : an Boeing 747 or an Airbus A380.\n",
    "*Spoiler :* This source code is 3 times better than Google AutoML on the same dataset !\n",
    "\n",
    "## Dataset\n",
    "It's an homemade dataset from Google Image.\n",
    "There is a nice script to get images from Google : https://github.com/boxabirds/fastai-helpers/blob/master/training-data-generator.py, it's based on https://github.com/hardikvasa/google-images-download wich use ChromeDriver to automate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '../../datasets/boeing_vs_airbus/tmp': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "PATH = \"../../datasets/boeing_vs_airbus/\"\n",
    "!rm -r {PATH}tmp\n",
    "## Getting dataset\n",
    "#!mkdir {PATH}\n",
    "#!pip install google_images_download\n",
    "#!ls ../../datasets/\n",
    "#!cd {PATH} && mkdir -p train/747 train/A380 test/747 test/A380  valid/747 valid/A380\n",
    "#!cd {PATH}/747 && googleimagesdownload --keywords \"Boing 747\" --limit 1000 --chromedriver /usr/local/bin/chromedriver\n",
    "#!cd {PATH}/A380 && googleimagesdownload --keywords \"Airbus A380\" --limit 1000 --chromedriver /usr/local/bin/chromedriver\n",
    "## Getting models weights\n",
    "#!cd ./fastai && wget http://files.fast.ai/models/weights.tgz && tar -xvzf weights.tgz\n",
    "#!ls {PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting CUDA devices...\n",
      "Loading Fast.AI modules...\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting CUDA devices...\")\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
    "print(\"Loading Fast.AI modules...\")\n",
    "# This file contains all the main external libs we'll use\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "# from fastai.transforms import *\n",
    "# from fastai.conv_learner import *\n",
    "# from fastai.model import *\n",
    "# from fastai.dataset import *\n",
    "# from fastai.sgdr import *\n",
    "# from fastai.plots import *\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sz=299\n",
    "arch=models.resnet50\n",
    "#arch=resnet34\n",
    "bs=132\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CuDNN optimization is  True\n"
     ]
    }
   ],
   "source": [
    "print(\"CuDNN optimization is \", torch.backends.cudnn.enabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training from sratch\n",
    "We begin with an empty Resnet50 model.\n",
    "\n",
    "## Computing mean and stddev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we compute the mean and standard deviation of all images to center them.\n",
    "It will help calculus to have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input images number :  1204\n",
      "stddev with PIL :  [1.61072035 1.57198706 1.66764579]\n",
      "Means with PIL :  [0.53317908 0.56569031 0.59960121]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import PIL\n",
    "means = np.array([0.0,0.0,0.0])\n",
    "files = [val for sublist in [[os.path.join(i[0], j) for j in i[2]] for i in os.walk(f'{PATH}train')] for val in sublist]\n",
    "print(\"Input images number : \", len(files))\n",
    "deviations = np.array([0.0,0.0,0.0])\n",
    "#variances = np.array([0,0,0])\n",
    "variance = np.array([0.0,0.0,0.0])\n",
    "for image in files:\n",
    "    #print(image)\n",
    "    img_stats = PIL.ImageStat.Stat(PIL.Image.open(image), mask=None)\n",
    "    means += np.array(img_stats.mean)\n",
    "    variance += np.array(img_stats.var)\n",
    "\n",
    "stddev = np.sqrt(variance) / len(files)\n",
    "print(\"stddev with PIL : \", stddev)\n",
    "\n",
    "mean = np.array(means) / len(files) / 255\n",
    "print(\"Means with PIL : \", mean)\n",
    "stats = (mean, stddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_23 october note :_ In the new version of fastai librairy you could do it in one line :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "normalize() missing 2 required positional arguments: 'mean' and 'std'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c885932acb73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagenet_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: normalize() missing 2 required positional arguments: 'mean' and 'std'"
     ]
    }
   ],
   "source": [
    "data.normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform.\n",
    "data = ImageDataBunch.from_folder(PATH, ds_tfms=get_transforms(), size=224)\n",
    "data.normalize(imagenet_stats)\n",
    "data.show_batch(rows=3, figsize=(7,6))\n",
    "learn = ConvLearner(data, arch, metrics=error_rate) #accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.summary()\n",
    "data.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is realy deep as it make use of 168 layers.\n",
    "\n",
    "How much input data do we get ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the label for a val data\n",
    "# plt.hist(data.train_ds.tfm_y)\n",
    "# plt.hist(data.val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size_d = {k: PIL.Image.open(PATH+k).size for k in data.trn_ds.fnames}\n",
    "# row_sz, col_sz = list(zip(*size_d.values()))\n",
    "# row_sz = np.array(row_sz)\n",
    "# col_sz = np.array(col_sz)\n",
    "# plt.hist(row_sz);\n",
    "# plt.hist(col_sz);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(row_sz[row_sz < 1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have few data, but sizes are good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guess learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrf=learn.lr_find(1e-6, end_lr=1)\n",
    "#learn.sched.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unfortunatly the learning rate finder do not work well with small dataset.\n",
    "_23 october note :_ In the new version of fastai librairy it seems to work !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_top_losses(9, figsize=(15,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix(figsize=(5,5), dpi=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.most_confused(min_val=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F2 :  0.58 for lr=1e-2, image_size=128, batch_size=150, epoch=30 # val_loss begin at 4.4 and acc stay at 0.50 \n",
    "#F2 :  0.02 for lr=1e-3, image_size=128, batch_size=256, epoch=30 # very erratic graph\n",
    "#F2 :  0.55 for lr=1e-4, image_size=128, batch_size=256, epoch=30\n",
    "#F2 :  0.11 for lr=1e-5, image_size=128, batch_size=256, epoch=30\n",
    "#F2 :  0.55 for lr=1e-4, image_size=128, batch_size=128, epoch=30\n",
    "#F2 :  0.49 (56.12 %) for lr=1e-4, image_size=128, batch_size=64\n",
    "#F2 :  0.55 (49.35 %) for lr=1e-4, image_size=128, batch_size=64\n",
    "\n",
    "lr=1e-4\n",
    "image_size=128\n",
    "batch_size=64\n",
    "epoch=30\n",
    "#data = ImageClassifierData.from_paths(PATH, tfms=tfms_from_model(arch, image_size), bs=batch_size, num_workers=num_cpus())\n",
    "data = ImageDataBunch.from_folder(PATH, ds_tfms=get_transforms(do_flip=False, flip_vert=False, max_rotate=0, max_zoom=0), size=224) # ds_tfms=get_transforms()\n",
    "data.normalize(imagenet_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn = ConvLearner(data=data, arch=arch, ps=0.5, pretrained=False, metrics=accuracy)\n",
    "learn.fit(epochs=epoch, lr=lr) # ,  get_ep_vals=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(\"After \", str(len(global_results)), \" epochs, the accuracy is \", str(vals_s2s[1]*100)[:5], \"%\" )\n",
    "#plot_ep_vals(global_results)\n",
    "#plot_the_confusion_matrix()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model had learn quite nothing. But it is not surprising with so small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r {PATH}tmp\n",
    "\n",
    "#F2 :  0.83 for image_size=128, batch_size=128, lr=1e-2\n",
    "# F2 :  0.80 for image_size=128, batch_size=150, lr=1e-3\n",
    "# F2 :  0.76 for image_size=128, batch_size=150, lr=1e-4 # good graph\n",
    "# F2 :  0.83 for image_size=128, batch_size=150, lr=1e-2 # val loss goes up => overfitting\n",
    "# F2 :  0.81 (acc 81.61 %) for image_size=128, batch_size=150, lr=1e-3 # val_loss go down, then slowly up\n",
    "# F2 :  0.76 (acc 76.45 %) for image_size=128, batch_size=64, lr=1e-4 # good graph, val_loss slowly go down\n",
    "# F2 :  0.76 (acc 77.74 %) for image_size=128, batch_size=64, lr=1e-4 # good graph, val_loss slowly go down\n",
    "\n",
    "image_size=128\n",
    "batch_size=64\n",
    "lr=1e-4\n",
    "epoch=30\n",
    "\n",
    "data = ImageDataBunch.from_folder(PATH, ds_tfms=get_transforms(do_flip=False, flip_vert=False, max_rotate=0, max_zoom=0), size=224) # ds_tfms=get_transforms()\n",
    "data.normalize(imagenet_stats)\n",
    "learn = ConvLearner(data=data, arch=arch, ps=0.5, pretrained=True, metrics=accuracy)\n",
    "learn.unfreeze()\n",
    "learn.fit(epochs=epoch, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tunning realy improve the accuracy, it is the first thing to do with any project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "Apply random operations on pictures to help the model to generalize.\n",
    "Data augmentation options : https://becominghuman.ai/data-augmentation-using-fastai-aefa88ca03f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug_tfms=[RandomRotate(10, tfm_y=TfmType.NO),\n",
    "#         RandomFlip(),\n",
    "#         RandomLighting(0.05, 0.05,tfm_y=TfmType.NO),\n",
    "#         RandomZoom(zoom_max=0.2),\n",
    "#         RandomStretch(max_stretch=0.2)]\n",
    "#tfms = tfms_from_model(arch, image_size, aug_tfms=aug_tfms, max_zoom=1.1)\n",
    "tfms = get_transforms(do_flip=True, flip_vert=False, max_rotate=10, max_zoom=1.1)\n",
    "\n",
    "\n",
    "# def get_augs():\n",
    "#     data = ImageDataBunch.from_folder(PATH, ds_tfms=tfms, size=224) # ds_tfms=get_transforms()\n",
    "#     x,_ = next(iter(data.train_dl))\n",
    "#     return data.train_dl.denorm(x)[1]\n",
    "\n",
    "# ims = np.stack([get_augs() for i in range(8)])\n",
    "# plots(ims, rows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r {PATH}tmp\n",
    "# F2 :  0.77 for image_size=128, batch_size=64, lr=1e-2 # val_loss don't go down\n",
    "# F2 :  0.83 for image_size=128, batch_size=128, lr=1e-3, epoch=30\n",
    "# F2 :  0.75 for image_size=128, batch_size=256, lr=1e-4, epoch=30\n",
    "# F2 :  0.81 for image_size=128, batch_size=64, lr=1e-4 # good graph\n",
    "# F2 :  0.85 (acc 84.19 %) for image_size=128, batch_size=64, lr=1e-3 # val_loss go slowy up\n",
    "# F2 :  0.79 (acc 76.45 %) for image_size=128, batch_size=64, lr=1e-4 # val_loss still go down\n",
    "# F2 :  0.84 (acc 85.16 %) for image_size=128, batch_size=64, lr=1e-3 # val_loss go slowy up\n",
    "\n",
    "image_size=128\n",
    "batch_size=64\n",
    "lr=1e-3\n",
    "epoch=30\n",
    "\n",
    "\n",
    "data = ImageDataBunch.from_folder(PATH, ds_tfms=tfms, size=224) # ds_tfms=get_transforms()\n",
    "data.normalize(imagenet_stats)\n",
    "learn = ConvLearner(data=data, arch=arch, ps=0.5, pretrained=True, metrics=accuracy)\n",
    "learn.unfreeze()\n",
    "learn.fit(epochs=epoch, lr=lr)\n",
    "\n",
    "# data = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=batch_size, num_workers=num_cpus())\n",
    "# learn = ConvLearner.pretrained(arch, data, precompute=False, ps=0.5, pretrained=True)\n",
    "# learn.unfreeze()\n",
    "# vals_s2s, global_results = learn.fit(lr, n_cycle=epoch, get_ep_vals=True)\n",
    "# print(\"After \", str(len(global_results)), \" epochs, the accuracy is \", str(vals_s2s[1]*100)[:5], \"%\" )\n",
    "# plot_ep_vals(global_results)\n",
    "# plot_the_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using data augmentation improve the accuracy by 10 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r {PATH}tmp\n",
    "#F2 :  0.82 for learning_rate = 1e-2, dropout = [0.25,0.5], image_size = 128, batch_size = 150 # val_loss goes up\n",
    "#F2 :  0.83 for learning_rate = 1e-2, dropout = 0.5, image_size = 128, batch_size = 150\n",
    "#F2 :  0.61 for learning_rate = 1e-4, dropout = 0.5, image_size = 128, batch_size = 150 # bad graph\n",
    "#F2 :  0.78 (acc 76.45 %) for learning_rate = 1e-3, dropout = [0.25,0.5], image_size = 128, batch_size = 150 # good graph\n",
    "#F2 :  0.81 (acc 82.25 %) for learning_rate = 1e-3, dropout = [0.25,0.5], image_size = 128, batch_size = 64 # good, val_loss goes down\n",
    "#F2 :  0.84 (acc 82.58 %) for learning_rate = 1e-2, dropout = [0.25,0.5], image_size = 128, batch_size = 64 # better acc but val_loss goes up\n",
    "\n",
    "\n",
    "learning_rate = 1e-3\n",
    "dropout = 0.5 #[0.25,0.5]\n",
    "image_size = 128\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "data = ImageDataBunch.from_folder(PATH, ds_tfms=tfms, size=224) # ds_tfms=get_transforms()\n",
    "data.normalize(imagenet_stats)\n",
    "learn = ConvLearner(data=data, arch=arch, ps=0.5, pretrained=True, metrics=accuracy)\n",
    "learn.unfreeze()\n",
    "\n",
    "training_loop = [\n",
    "    [123, 64, 10],\n",
    "    [150, 128, 10],\n",
    "    [123, 224, 10],\n",
    "]\n",
    "for bs, sz, epoch in training_loop:\n",
    "    data.batch_size = bs\n",
    "    learn.fit(epochs=epoch, lr=lr)\n",
    "\n",
    "# global_results = collections.OrderedDict([])\n",
    "# aug_tfms=[RandomRotate(10, tfm_y=TfmType.NO), RandomFlip(), RandomLighting(0.05, 0.05,tfm_y=TfmType.NO), RandomZoom(zoom_max=0.2),RandomStretch(max_stretch=0.2)]\n",
    "# tfms = tfms_from_model(arch, image_size, aug_tfms=aug_tfms, max_zoom=1.1)\n",
    "# data = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=batch_size, num_workers=num_cpus())\n",
    "# learn = ConvLearner.pretrained(arch, data, precompute=False, ps=dropout, pretrained=True)\n",
    "# learn.unfreeze()\n",
    "# training_loop = [\n",
    "#     [123, 300, 1],\n",
    "#     [512, 64, 10],\n",
    "#     [150, 128, 10],\n",
    "#     [123, 300, 10],\n",
    "# ]\n",
    "\n",
    "# for bs, sz, epoch in training_loop:\n",
    "#     lr=np.array([learning_rate/100,learning_rate/10,learning_rate]) # Learning rate plus faible pour les premières couche, pour ré-apprendre un peu si necessaire\n",
    "#     tfms = tfms_from_model(arch, sz, aug_tfms=aug_tfms, max_zoom=0.5)\n",
    "#     data = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs, num_workers=num_cpus())\n",
    "#     vals_s2s, ep_vals_s2s = learn.fit(lr, n_cycle=epoch, get_ep_vals=True)\n",
    "#     if len(global_results) > 0:\n",
    "#         #print(global_results)\n",
    "#         for k, v in ep_vals_s2s.items():\n",
    "#             global_results[len(global_results)] = v\n",
    "#     else:\n",
    "#         global_results = ep_vals_s2s\n",
    "#     print(\"After \", str(len(global_results)), \" epochs, the accuracy is \", str(vals_s2s[1]*100)[:5], \"%\" )\n",
    "#     fichier = arch.__name__ + '_' + str(len(global_results)) + \"_\" + str(sz) + \"_acc\" + str(vals_s2s[1]*100)[:5] + '_weights'\n",
    "#     print(\"Saving to \", fichier)\n",
    "#     learn.save(fichier)\n",
    "# plot_ep_vals(global_results)\n",
    "# plot_the_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, using multiple size did not realy help but it is something to try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDR - Learning rate anealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r {PATH}tmp\n",
    "\n",
    "# SGDR : Earlier we said 3 is the number of epochs, but it is actually *cycles*. So if cycle_len=2 , it will do 3 cycles where each cycle is 2 epochs (i.e. 6 epochs). Then why did it 7? It is because of cycle_mult :\n",
    "#     cycle_mult=2 : this multiplies the length of the cycle after each cycle (1 epoch + 2 epochs + 4 epochs = 7 epochs).\n",
    "\n",
    "# F2 :  0.81 for learning_rate = 1e-4, lr=np.array([learning_rate/100,learning_rate/10,learning_rate]), dropout = [0.25,0.5] \n",
    "# F2 :  0.91 for learning_rate = 1e-4, lr=np.array([learning_rate/5,learning_rate/3,learning_rate]), dropout = [0.25,0.5] # good graph\n",
    "# F2 :  0.93 (acc 94.51 %) for learning_rate = 1e-3, lr=np.array([learning_rate/5,learning_rate/3,learning_rate]), dropout = [0.25,0.5]\n",
    "# F2 :  0.69 (acc 69.35 %) for learning_rate = 1e-2, lr=np.array([learning_rate/5,learning_rate/3,learning_rate]), dropout = [0.25,0.5] # graphs erratic but goes down\n",
    "# F2 :  0.93 (acc 91.93 %) for learning_rate = 1e-3, lr=np.array([learning_rate/5,learning_rate/3,learning_rate]), dropout = [0.25,0.5] # graphs erratic at the end but goes down\n",
    "# F2 :  0.94 (acc 93.54 %) for learning_rate = 1e-3, lr=np.array([learning_rate/5,learning_rate/3,learning_rate]), dropout = 0.5\n",
    "\n",
    "\n",
    "learning_rate = 1e-3\n",
    "lr=np.array([learning_rate/5,learning_rate/3,learning_rate])\n",
    "dropout = 0.5\n",
    "#[0.25,0.5] \n",
    "\n",
    "\n",
    "data = ImageDataBunch.from_folder(PATH, ds_tfms=tfms, size=224) # ds_tfms=get_transforms()\n",
    "data.normalize(imagenet_stats)\n",
    "learn = ConvLearner(data=data, arch=arch, ps=0.5, pretrained=True, metrics=accuracy)\n",
    "learn.unfreeze()\n",
    "\n",
    "training_loop = [\n",
    "    [123, 64, 10],\n",
    "    [150, 128, 10],\n",
    "    [123, 224, 10],\n",
    "]\n",
    "for bs, sz, epoch in training_loop:\n",
    "    data.batch_size = bs\n",
    "    learn.fit_one_cycle(cyc_len=epoch, max_lr=lr)\n",
    "\n",
    "# aug_tfms=[RandomRotate(10, tfm_y=TfmType.NO),RandomFlip(),RandomLighting(0.05, 0.05,tfm_y=TfmType.NO), RandomZoom(zoom_max=0.2), RandomStretch(max_stretch=0.2)]\n",
    "# tfms = tfms_from_model(arch, sz, aug_tfms=aug_tfms, max_zoom=1.1)\n",
    "# data = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=8, num_workers=num_cpus())\n",
    "# learn = ConvLearner.pretrained(arch, data, precompute=False, ps=0.5)\n",
    "# learn.unfreeze()\n",
    "# learn.ps=dropout\n",
    "# global_results = collections.OrderedDict([])\n",
    "\n",
    "# training_loop = [\n",
    "#     [512, 64, 10],\n",
    "#     [150, 128, 10],\n",
    "#     [123, 300, 10],\n",
    "# ]\n",
    "\n",
    "# for bs, sz, epoch in training_loop:\n",
    "#     print(\"Hyperparameters : Batch size=\", bs, \" Drop out=\", dropout, \" Learning rate=\", learning_rate, \" Cycle=\", epoch, \" Images sizes=\", sz )\n",
    "#     tfms = tfms_from_model(arch, sz, aug_tfms=aug_tfms, max_zoom=0.5)\n",
    "#     # On recharge les données avec une taille de batch plus importante pour aller plus vite\n",
    "#     data = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs, num_workers=num_cpus())\n",
    "#     vals_s2s, ep_vals_s2s = learn.fit(lr, n_cycle=epoch, cycle_len=1, get_ep_vals=True)\n",
    "#     if len(global_results) > 0:\n",
    "#         for k, v in ep_vals_s2s.items(): global_results[len(global_results)] = v\n",
    "#     else:\n",
    "#         global_results = ep_vals_s2s\n",
    "#     print(\"After \", str(len(global_results)), \" epochs, the accuracy is \", str(vals_s2s[1]*100)[:5], \"%\" )\n",
    "#     fichier = arch.__name__ + '_' + str(len(global_results)) + \"_\" + str(sz) + \"_acc\" + str(vals_s2s[1]*100)[:5] + '_weights'\n",
    "#     print(\"Saving to \", fichier)\n",
    "#     learn.save(fichier)\n",
    "# plot_ep_vals(global_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_the_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that give us another XXX points improvement !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable cycle length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r {PATH}tmp\n",
    "\n",
    "learning_rate = 1e-3\n",
    "lr=np.array([learning_rate/10,learning_rate/5,learning_rate])\n",
    "dropout = 0.5\n",
    "#F2 :  0.96 for learning_rate = 1e-3, lr=np.array([learning_rate/10,learning_rate/5,learning_rate]), dropout = 0.5\n",
    "#F2 :  0.92 for learning_rate = 1e-3, lr=np.array([learning_rate/10,learning_rate/5,learning_rate]), dropout = [0.25,0.5]\n",
    "#F2 :  0.86 for learning_rate = 1e-2, lr=np.array([learning_rate/10,learning_rate/5,learning_rate]), dropout = 0.5\n",
    "#F2 :  0.88 (acc 90    %) for learning_rate = 1e-3, lr=np.array([learning_rate/5,learning_rate/3,learning_rate]), dropout = 0.5 # graph go slowly down, better result at 29 epoch than 44\n",
    "#F2 :  0.96 (acc 94.83 %) for learning_rate = 1e-3, lr=np.array([learning_rate/10,learning_rate/5,learning_rate]), dropout = 0.5\n",
    "#F2 :  0.94 (acc 91.93 %) for learning_rate = 1e-3, lr=np.array([learning_rate/50,learning_rate/10,learning_rate]), dropout = 0.5\n",
    "#F2 :  0.93 (acc 90.96 %) for learning_rate = 1e-3, lr=np.array([learning_rate/15,learning_rate/6,learning_rate]), dropout = 0.5\n",
    "\n",
    "\n",
    "# aug_tfms=[RandomRotate(10, tfm_y=TfmType.NO),RandomFlip(),RandomLighting(0.05, 0.05,tfm_y=TfmType.NO), RandomZoom(zoom_max=0.2), RandomStretch(max_stretch=0.2)]\n",
    "# tfms = tfms_from_model(arch, sz, aug_tfms=aug_tfms, max_zoom=1.1)\n",
    "# data = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=8, num_workers=num_cpus())\n",
    "# learn = ConvLearner.pretrained(arch, data, precompute=False, ps=0.5)\n",
    "# learn.unfreeze()\n",
    "# learn.ps=dropout\n",
    "# global_results = collections.OrderedDict([])\n",
    "\n",
    "# training_loop = [\n",
    "#     [512, 64, 4],\n",
    "#     [256, 128, 3],\n",
    "#     [123, 300, 4]\n",
    "# ]\n",
    "# for bs, sz, cycle in training_loop:\n",
    "#     print(\"Hyperparameters : Batch size=\", bs, \" Drop out=\", dropout, \" Learning rate=\", learning_rate, \" Cycle=\", cycle, \" Images sizes=\", sz )\n",
    "#     tfms = tfms_from_model(arch, sz, aug_tfms=aug_tfms, max_zoom=0.5)\n",
    "#     data = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs, num_workers=num_cpus())\n",
    "#     vals_s2s, ep_vals_s2s = learn.fit(lr, cycle, cycle_len=1, cycle_mult=2, get_ep_vals=True)\n",
    "#     if len(global_results) > 0:\n",
    "#         for k, v in ep_vals_s2s.items(): global_results[len(global_results)] = v\n",
    "#     else:\n",
    "#         global_results = ep_vals_s2s\n",
    "#     print(\"After \", str(len(global_results)), \" epochs, the accuracy is \", str(vals_s2s[1]*100)[:5], \"%\" )\n",
    "#     fichier = arch.__name__ + '_' + str(len(global_results)) + \"_\" + str(sz) + \"_acc\" + str(vals_s2s[1]*100)[:5] + '_weights'\n",
    "#     print(\"Saving to \", fichier)\n",
    "#     learn.save(fichier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(num=None, figsize=(14, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "plot_ep_vals(global_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix(figsize=(12,12), dpi=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(9, figsize=(15,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('lesson01-final_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 2 points improvement but it's great as we are approching Google AutoML performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loadmodel=False\n",
    "loadmodel=True\n",
    "if loadmodel==True:\n",
    "    data = ImageDataBunch.from_folder(PATH, ds_tfms=get_transforms(), size=224)\n",
    "    learn = ConvLearner(data=data, arch=arch, ps=0.5, pretrained=True, metrics=accuracy)\n",
    "    #learn = ConvLearner.pretrained(arch, data, precompute=False, ps=0.5)\n",
    "    learn.load(\"lesson01-final_model\")\n",
    "    print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction et visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix(figsize=(5,5), dpi=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(9, figsize=(15,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall\n",
    "Précision et rappel en français : https://fr.wikipedia.org/wiki/Pr%C3%A9cision_et_rappel\n",
    "_La précision est le nombre de documents pertinents retrouvés rapporté au nombre de documents total proposé par le moteur de recherche pour une requête donnée. _ Autrement dit quelle proportion d'identifications positives était effectivement correcte ?\n",
    "\n",
    "_Le rappel est défini par le nombre de documents pertinents retrouvés au regard du nombre de documents pertinents que possède la base de données._ Autrement dit quelle proportion de résultats positifs réels a été identifiée correctement ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.classes\n",
    "cm = interp.confusion_matrix()\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# precission = VP / (VP + FP)\n",
    "precision = cm[0][0]/(cm[0][1]+cm[0][0]);\n",
    "print(\"Precision : \", precision)\n",
    "# recall =  = VP / (VP + FN)\n",
    "recall = cm[0][0]/(cm[1][0]+cm[0][0]);\n",
    "print(\"recall : \", recall)\n",
    "F1 = 2 * ((precision*recall)/(precision + recall))\n",
    "print(\"F1 : \", F1)\n",
    "F2 = 5 * ((precision*recall)/(4*precision + recall))\n",
    "print(\"F2 : \", F2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(16, figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est bon, les plus petites font plus de 200 pixels.\n",
    "On note que la majorité on une taille importante, il sera bon de faire des entrainement du modèle avec des tailles supérieures.\n",
    "\n",
    "_\"How many images should we use as a validation set? [01:26:28] Using 20% is fine unless the dataset is small — then 20% is not enough. If you train the same model multiple times and you are getting very different validation set results, then your validation set is too small. If the validation set is smaller than a thousand, it is hard to interpret how well you are doing. If you care about the third decimal place of accuracy and you only have a thousand things in your validation set, a single image changes the accuracy. If you care about the difference between 0.01 and 0.02, you want that to represent 10 or 20 rows. Normally 20% seems to work fine.\"_\n",
    "=> Nous avons environs 100 images de validation par classes, ça devrait donc aller.\n",
    "\n",
    "Faisons un test d'entrainement avec une petite taille de batch pour estimer la valeur du learning rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FastAIv3",
   "language": "python",
   "name": "fastaiv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
